# -*- coding: utf-8 -*-
"""Final_Thesis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iuRdIh4UYNP7MsiA3Vy3wJ1AbNXrFVI2
"""

!pip install librosa
!pip install tensorflow

!pip install resampy
!pip install -U librosa resampy soundfile

import os
import numpy as np
import librosa
import librosa.display
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Flatten, Dense,
                                     LSTM, Bidirectional, TimeDistributed, Dropout)
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder

def extract_features(file_path, n_mfcc=40, max_pad_len=200):
    # Load audio file
    audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')

    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)

    pad_width = max_pad_len - mfccs.shape[1]
    if pad_width > 0:
        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')
    else:
        mfccs = mfccs[:, :max_pad_len]
    return mfccs

def load_ravdess_data(directory):
    features = []
    emotions = []
    intensities = []

    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(".wav"):
                file_path = os.path.join(root, file)

                feature = extract_features(file_path)
                features.append(feature)


                parts = file.split('-')
                emotion = int(parts[2])
                intensity = int(parts[3])

                emotions.append(emotion)
                intensities.append(intensity)

    return np.array(features), np.array(emotions), np.array(intensities)


directory = "/content/drive/MyDrive/Dataset"
features, emotions, intensities = load_ravdess_data(directory)

#print(features)

# Mapping for emotions in the RAVDESS dataset
emotions = {
    '1': 'neutral',
    '2': 'calm',
    '3': 'happy',
    '4': 'sad',
    '5': 'angry',
    '6': 'fearful',
    '7': 'disgust',
    '8': 'surprised'
}

# Intensity Mapping
intensity = {
    '1': 'normal',
    '2': 'strong'
}

# # Observed emotions and intensities
# observed_emotions = ['calm', 'happy', 'fearful', 'disgust','angry',]
# observed_intensities = ['normal', 'strong']

train_directory = "/content/drive/MyDrive/Dataset/Train"  # Replace with your training dataset path
test_directory = "/content/drive/MyDrive/Dataset/Test"    # Replace with your test dataset path


X_train, y_emotion_train, y_intensity_train = load_ravdess_data(train_directory)


X_test, y_emotion_test, y_intensity_test = load_ravdess_data(test_directory)

# Normalize features
X_train = X_train / np.max(np.abs(X_train))
X_test = X_test / np.max(np.abs(X_test))


X_train = X_train[..., np.newaxis]
X_test = X_test[..., np.newaxis]

# One-hot encode labels
emotion_encoder = LabelEncoder()
y_emotion_train = to_categorical(emotion_encoder.fit_transform(y_emotion_train))
y_emotion_test = to_categorical(emotion_encoder.transform(y_emotion_test))

intensity_encoder = LabelEncoder()
y_intensity_train = to_categorical(intensity_encoder.fit_transform(y_intensity_train))
y_intensity_test = to_categorical(intensity_encoder.transform(y_intensity_test))

def build_hybrid_model(input_shape, num_emotions, num_intensities):

    inputs = Input(shape=input_shape)

    # CNN layers
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2))(x)
    x = Dropout(0.3)(x)
    x = TimeDistributed(Flatten())(x)

    # RNN layers
    x = Bidirectional(LSTM(128, return_sequences=False))(x)


    emotion_output = Dense(num_emotions, activation='softmax', name='emotion_output')(x)


    intensity_output = Dense(num_intensities, activation='softmax', name='intensity_output')(x)

    # Model
    model = Model(inputs=inputs, outputs=[emotion_output, intensity_output])
    return model

# Build the model
input_shape = X_train.shape[1:]  # (n_mfcc, max_pad_len, 1)
num_emotions = y_emotion_train.shape[1]
num_intensities = y_intensity_train.shape[1]

model = build_hybrid_model(input_shape, num_emotions, num_intensities)


model.compile(optimizer='adam',
              loss={'emotion_output': 'categorical_crossentropy',
                    'intensity_output': 'categorical_crossentropy'},
              loss_weights={'emotion_output': 1.0, 'intensity_output': 0.5},
              metrics={'emotion_output': ['accuracy'],
                       'intensity_output': ['accuracy']})  # Specify metrics for each output

model.summary()

history = model.fit(
    X_train,
    {'emotion_output': y_emotion_train, 'intensity_output': y_intensity_train},
    validation_data=(X_test, {'emotion_output': y_emotion_test, 'intensity_output': y_intensity_test}),
    epochs=30,
    batch_size=30,
    verbose=1
)

# Evaluate the model on the test dataset
evaluation = model.evaluate(
    X_test,
    {'emotion_output': y_emotion_test, 'intensity_output': y_intensity_test},
    verbose=1
)

# Extract and print accuracies
emotion_accuracy = evaluation[3]  # Index 3 corresponds to 'emotion_output_accuracy'
intensity_accuracy = evaluation[4]  # Index 4 corresponds to 'intensity_output_accuracy'

print(f"Emotion Recognition Accuracy: {emotion_accuracy * 100:.2f}%")
print(f"Intensity Classification Accuracy: {intensity_accuracy * 100:.2f}%")

print(evaluation)
print(len(evaluation))

# The following block is modified to handle cases where evaluation might not have the expected length
if len(evaluation) > 3:
    emotion_accuracy = evaluation[3]
else:
    emotion_accuracy = None  # or a default value like 0.0

if len(evaluation) > 4:
    intensity_accuracy = evaluation[4]
else:
    intensity_accuracy = None  # or a default value like 0.0

# Assuming evaluation is [loss, output_1_accuracy, output_2_accuracy]
# if len(evaluation) >= 3:
#     emotion_accuracy = evaluation[1]
#     intensity_accuracy = evaluation[2]
# else:
#     print("Evaluation output is smaller than expected!")


# print(f"Emotion Recognition Accuracy: {emotion_accuracy * 100:.2f}%")
# print(f"Intensity Classification Accuracy: {intensity_accuracy * 100:.2f}%")

def predict_emotion_and_intensity(model, file_path, emotion_encoder, intensity_encoder, n_mfcc=40, max_pad_len=200):

    features = extract_features(file_path, n_mfcc=n_mfcc, max_pad_len=max_pad_len)
    features = features / np.max(np.abs(features))
    features = features[np.newaxis, ..., np.newaxis]


    predictions = model.predict(features)


    emotion_prediction = np.argmax(predictions[0], axis=1)[0]
    intensity_prediction = np.argmax(predictions[1], axis=1)[0]


    predicted_emotion = emotion_encoder.inverse_transform([emotion_prediction])[0]
    predicted_intensity = intensity_encoder.inverse_transform([intensity_prediction])[0]

    return predicted_emotion, predicted_intensity

# def batch_predict(model, directory, emotion_encoder, intensity_encoder, n_mfcc=40, max_pad_len=200):
#     results = []
#     print(f"Looking for .wav files in: {directory}")
#     for file in os.listdir(directory):
#         if file.endswith('.wav'):
#             try:
#                 file_path = os.path.join(directory, file)
#                 print(f"Processing file: {file_path}")
#                 # Get emotion and intensity predictions
#                 emotion, intensity = predict_emotion_and_intensity(
#                     model, file_path, emotion_encoder, intensity_encoder, n_mfcc, max_pad_len
#                 )
#                 # Append categorical results
#                 results.append((file, emotion, intensity))
#             except Exception as e:
#                 print(f"Error processing file {file}: {e}")
#     return results

# # Directory containing test audio files
# test_audio_directory = "/content/drive/MyDrive/Dataset/Train/Actor_01"

# # Predict for all files in the directory
# predictions = batch_predict(model, test_audio_directory, emotion_encoder, intensity_encoder)

# # Print the categorical results
# if predictions:
#     print("\nPredictions:")
#     for file, emotion, intensity in predictions:
#         print(f"File: {file}, Predicted Emotion: {emotion+1}, Predicted Intensity: {intensity+1}")
# else:
#     print("No predictions were made. Check the directory or the files.")

# Mapping for emotions and intensity in the RAVDESS dataset
emotions = {
    '1': 'neutral',
    '2': 'calm',
    '3': 'happy',
    '4': 'sad',
    '5': 'angry',
    '6': 'fearful',
    '7': 'disgust',
    '8': 'surprised'
}

intensity_mapping = {
    '1': 'normal',
    '2': 'strong'
}

def decode_predictions(emotion_idx, intensity_idx):

    emotion_label = emotions.get(str(emotion_idx), "unknown")
    intensity_label = intensity_mapping.get(str(intensity_idx), "unknown")
    return emotion_label, intensity_label

def batch_predict(model, directory, emotion_encoder, intensity_encoder, n_mfcc=40, max_pad_len=200):
    results = []
    print(f"Looking for .wav files in: {directory}")
    for file in os.listdir(directory):
        if file.endswith('.wav'):
            try:
                file_path = os.path.join(directory, file)
                print(f"Processing file: {file_path}")

                emotion_idx, intensity_idx = predict_emotion_and_intensity(
                    model, file_path, emotion_encoder, intensity_encoder, n_mfcc, max_pad_len
                )

                emotion, intensity = decode_predictions(emotion_idx, intensity_idx)

                results.append((file, emotion, intensity))
            except Exception as e:
                print(f"Error processing file {file}: {e}")
    return results


test_audio_directory = "/content/drive/MyDrive/Dataset/Train/Actor_02"


predictions = batch_predict(model, test_audio_directory, emotion_encoder, intensity_encoder)


if predictions:
    print("\nPredictions:")
    for file, emotion, intensity in predictions:
        print(f"File: {file}, Predicted Emotion: {emotion}, Predicted Intensity: {intensity}")
else:
    print("No predictions were made. Check the directory or the files.")

# save_path = "/content/drive/MyDrive/Dataset"

# model.save(f"{save_path}emotion_intensity_model.h5")
# print(f"Model saved at: {save_path}emotion_intensity_model.h5")

import os

save_path = "/content/sample_data"
if not os.path.exists(save_path):
    print("The folder does not exist. Please check the path.")
else:
    print("Folder exists:", save_path)

model.save(f"{save_path }emotion_intensity_model.h5")
print(f"Model saved successfully at: {save_path}emotion_intensity_model.h5")

predictions = model.predict(X_test)
emotion_predictions = predictions[0]
intensity_predictions = predictions[1]

# Convert probabilities to class labels for each output
predicted_emotions = np.argmax(emotion_predictions, axis=1)
predicted_intensities = np.argmax(intensity_predictions, axis=1)

actual_emotions = np.argmax(y_emotion_test, axis=1)
actual_intensities = np.argmax(y_intensity_test, axis=1)

print("Emotion Predictions vs Actuals:")
for actual, predicted in zip(actual_emotions, predicted_emotions):
    print(f"Actual Emotion: {actual}, Predicted Emotion: {predicted}")

print("\nIntensity Predictions vs Actuals:")
for actual, predicted in zip(actual_intensities, predicted_intensities):
    print(f"Actual Intensity: {actual}, Predicted Intensity: {predicted}")

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt


emotion_classes = ['neutral', 'calm', 'Happy', 'Sad', 'angry','fearful','disgust', 'surprised']  # Replace with your actual emotion class names

# Emotion confusion matrix
cm_emotion = confusion_matrix(actual_emotions, predicted_emotions)
print("Emotion Confusion Matrix:")
print(cm_emotion)

# Intensity confusion matrix
cm_intensity = confusion_matrix(actual_intensities, predicted_intensities)
print("\nIntensity Confusion Matrix:")
print(cm_intensity)



intensity_classes = ['normal', 'strong']

# Visualize confusion matrices

plt.figure(figsize=(8, 6))
sns.heatmap(cm_emotion, annot=True, fmt="d", cmap="Blues", xticklabels=emotion_classes, yticklabels=emotion_classes)
plt.title("Emotion Confusion Matrix")
plt.xlabel("Predicted Emotion")
plt.ylabel("Actual Emotion")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(cm_intensity, annot=True, fmt="d", cmap="Greens", xticklabels=intensity_classes, yticklabels=intensity_classes)
plt.title("Intensity Confusion Matrix")
plt.xlabel("Predicted Intensity")
plt.ylabel("Actual Intensity")
plt.show()

!pip install gradio

import gradio as gr
import numpy as np
from tensorflow.keras.models import load_model
import librosa
import pickle
import os

# Define the path to your model file
model_path = "/content/sample_dataemotion_intensity_model.h5"

# Load your trained model
model = load_model(model_path)




# Emotion and intensity mappings
emotions = {
    '1': 'neutral',
    '2': 'calm',
    '3': 'happy',
    '4': 'sad',
    '5': 'angry',
    '6': 'fearful',
    '7': 'disgust',
    '8': 'surprised'
}
intensity = {
    '1': 'normal',
    '2': 'strong'
}

# Preprocessing function for audio
def preprocess_audio(file_path, n_mfcc=40, max_pad_len=200):
    try:
        audio, sr = librosa.load(file_path, sr=None)
        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)
        if mfcc.shape[1] < max_pad_len:
            pad_width = max_pad_len - mfcc.shape[1]
            mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')
        else:
            mfcc = mfcc[:, :max_pad_len]
        return mfcc[np.newaxis, ..., np.newaxis]  # Add channel dimension
    except Exception as e:
        raise ValueError(f"Error processing audio: {e}")

# Prediction function
def predict_emotion_and_intensity(audio_file):
    # Preprocess audio
    features = preprocess_audio(audio_file)

    # Predict using the model
    predictions = model.predict(features)

    # Decode predictions
    emotion_pred = np.argmax(predictions[0]) + 1  # +1 to match mapping
    intensity_pred = np.argmax(predictions[1]) + 1  # +1 to match mapping

    emotion_result = emotions[str(emotion_pred)]
    intensity_result = intensity[str(intensity_pred)]

    return f"Emotion: {emotion_result}, Intensity: {intensity_result}"

with gr.Blocks() as gradio_app:
    gr.Markdown("## Speech Emotion and Intensity Classifier")
    gr.Markdown("Upload a `.wav` file to classify emotion and intensity.")

    with gr.Row():
        # Change 'source' to 'type' to specify the input method
        audio_input = gr.Audio(type="filepath", label="Upload Audio File (.wav)")
        output_text = gr.Textbox(label="Prediction", lines=2)

    submit_button = gr.Button("Classify")
    submit_button.click(predict_emotion_and_intensity, inputs=audio_input, outputs=output_text)

# Launch the app
gradio_app.launch()

